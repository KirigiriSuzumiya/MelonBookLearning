# 第四章 决策树

2022/5/26 雾切凉宫 至4.2节/视频P6



[TOC]

## 4.1 基本流程

### p6 决策树

#### p6.1 算法原理

从**逻辑**角度，-堆if else语句的组合
从**几何**角度，根据某种准则划分特征空间
最终目的:将样本越分越“**纯**”



## 4.2划分选择

#### p6.2 ID3决策树

##### 自信息

$$
I(X)=-log_bp(x)
$$

当b = 2时单位为bit，当b = e时单位为nat （仅仅是个单位）



##### 信息熵(自信息的期望) 

度量随机变量X的**不确定性**，信息熵越大越不确定
$$
H(x)=E[I(x)]=-\sum_xp(x)log_bp(x)
$$
当X的某个取值的概率为1时信息熵最小(**最确定**)，其值为0;当X的各个取值的概率均等时信息熵最大(**最不确定**)，其值为logb |X|,其中|X|表示X可能取值的个数。



##### 纯度

将样本类别标记**y**视作随机变量，各个类别在样本集合**D**中的占比**pk**视作各个类别取值的概率，则样本集合D (随机变量y)的**信息熵**(底数b取2) 为：
$$
Ent(D)=H(y)=-\sum^{|y|}_{k=1}p_klog_2p_k
$$
此时的信息熵所代表的“不确定性”可以转换理解为集合内样本的**“纯度”**。Ent(D)的值越小，D的纯度越高。



##### 条件熵

条件熵(**Y**的信息熵关于**随机分布X**的期望) :在已知X后Y的不确定性
$$
H(Y|X)=\sum_xp(x)H(Y|X=x)
$$
Dv表示属性a取值属于{a1,a2,…av}的样本集合，在已知属性a的取值后，样本集合D的条件熵为：
$$
\sum^V_{v=1}\frac{{|D^v|}}{{|D|}}Ent(D^v)
$$


##### 信息增益

信息增益:在已知属性(特征) a的取值后y的不确定性减少的量，也即**纯度的提升**
$$
Gain(D,a)=Ent(D)-\sum^V_{v=1}\frac{D^v}{D}Ent(D^v)
$$

##### ID3决策树

以信息增益为准则来选择划分属性的决策树
$$
a_*={argmax}Gain(D,a)
$$
**说人话**：在所有属性中找到**信息增益（Gain）最大**的**属性值（a）**作为决策树的决策变量。



#### p6.3 C4.5决策树

信息增益准则对可能取值数目较多的属性有所偏好(例如“编号”这个较为极端的例子，不过其本质原因不是取值数目过多，而是每个取值里面所包含的样本量太少)，为减少这种偏好可能带来的不利影响，C4.5决策树选择使用**“增益率"**代替**“信息增益”**

##### 增益率

增益率定义为：
$$
Gain_ratio(D,a)=\frac{Gain(D,a)}{IV(a)}
$$
其中：
$$
IV(a)=\sum_{v=1}^V\frac{{|D^v|}}{{|D|}}log_2\frac{D^v}{D}
$$
称为属性a的**“固有值”**，a的可能取值个数V越大，通常其固有值IV(a)也越大。但是,**增益率**对可能取值**数目较少**的属性有所偏好。

因此，C4. 5决策树并未完全使用“增益率"代替“信息增益”，而是采用一种**启发式**的方法:
先选出**信息增益高于平均水平**的属性，然后再从中选择**增益率最高**的。



#### p6.4 CART决策树

##### 基尼值

从样本集合D中随机抽取两个样本，其类别标记不一致的概率。因此，基尼值越小，碰到异类的概率就越小，纯度自然就越高。
$$
Gini(D)=\sum_{k=1}^{{|y|}}p_k(1-p_k)=1-\sum_{k=1}^{{|y|}}p_k^2
$$

##### 基尼指数

属性a的基尼指数，类似条件熵：
$$
Gini\_index(D,a)=\sum_{v=1}^V\frac{{|D^v|}}{{|D|}}Gini(D^v)
$$

##### CART决策树

选择**基尼指数最小**的属性作为最优划分属性
$$
a_*=argminGini\_index(D,a)
$$
CART决策树的实际**构造算法**如下:

- 首先，对每个属性a的每个可能取值v，将数据集D分为a=v和a≠y两部分来计算基尼指数，即:
  $$
  Gini\_index(D,a)=\frac{|{D^{a=v}}|}{{|D|}}Gini(D^{a=v})+\frac{|{D^{a\not=v}}|}{|{D}|}Gini(D^{a\not=v})
  $$
  
- 然后，选择基尼指数最小的属性及其对应取值作为最优划分属性和最优划分点;

- 最后，重复以上两步，直至满足停止条件。
